from pyspark.sql import functions as F, types as T

RAW_TABLE = "puffy.raw_events"

def basic_schema_validation(df):
    expected_cols = [
        "client_id", "page_url", "referrer",
        "timestamp", "event_name", "event_data", "user_agent"
    ]
    missing_cols = [c for c in expected_cols if c not in df.columns]
    extra_cols = [c for c in df.columns if c not in expected_cols]
    return missing_cols, extra_cols

def null_and_empty_checks(df):
    return (
        df.select(
            F.count(F.when(F.col("client_id").isNull(), 1)).alias("null_client_id"),
            F.count(F.when(F.trim(F.col("page_url")) == "", 1)).alias("empty_page_url"),
            F.count(F.when(F.col("timestamp").isNull(), 1)).alias("null_timestamp"),
            F.count(F.when(F.trim(F.col("event_name")) == "", 1)).alias("empty_event_name")
        )
    )

def timestamp_format_check(df):
    return df.withColumn(
        "parsed_ts",
        F.to_timestamp("timestamp")
    ).select(
        F.count(F.when(F.col("parsed_ts").isNull(), 1)).alias("bad_timestamp_format")
    )

def allowed_event_names_check(df):
    allowed = [
        "page_viewed",
        "email_filled_on_popup",
        "add_to_cart",
        "begin_checkout",
        "purchase"
    ]
    return df.select(
        F.count(F.when(~F.col("event_name").isin(allowed), 1)).alias("unknown_events")
    )

def duplicate_event_check(df):
    window_cols = ["client_id", "timestamp", "event_name", "page_url"]
    dupes = (
        df.groupBy(*window_cols)
          .count()
          .filter(F.col("count") > 1)
    )
    return dupes

def event_data_json_check(df):
    return df.select(
        F.count(
            F.when(
                (F.col("event_data").isNotNull()) &
                (F.col("event_data") != "") &
                F.json_tuple("event_data", "dummy").isNull(), 1
            )
        ).alias("malformed_event_data_json")
    )

def run_all_checks(spark, source_path, save_table="puffy.quality_event_issues"):
    df = (
        spark.read.option("header", True)
                  .option("multiLine", True)
                  .option("escape", "\"")
                  .csv(source_path)
    )

    missing_cols, extra_cols = basic_schema_validation(df)

    nulls = null_and_empty_checks(df).collect()[0]
    ts_issues = timestamp_format_check(df).collect()[0]
    unknown = allowed_event_names_check(df).collect()[0]
    malformed = event_data_json_check(df).collect()[0]
    dupes = duplicate_event_check(df)

    summary_rows = [
        ("missing_columns", ",".join(missing_cols) if missing_cols else None),
        ("extra_columns", ",".join(extra_cols) if extra_cols else None),
        ("null_client_id", str(nulls["null_client_id"])),
        ("empty_page_url", str(nulls["empty_page_url"])),
        ("null_timestamp", str(nulls["null_timestamp"])),
        ("empty_event_name", str(nulls["empty_event_name"])),
        ("bad_timestamp_format", str(ts_issues["bad_timestamp_format"])),
        ("unknown_events", str(unknown["unknown_events"])),
        ("malformed_event_data_json", str(malformed["malformed_event_data_json"]))
    ]

    summary_schema = T.StructType([
        T.StructField("check_name", T.StringType(), False),
        T.StructField("check_value", T.StringType(), True),
    ])

    summary_df = spark.createDataFrame(summary_rows, summary_schema)

    (summary_df
        .write.mode("overwrite")
        .saveAsTable(save_table))

    (dupes
        .write.mode("overwrite")
        .saveAsTable("puffy.quality_event_duplicates"))

    return summary_df, dupes
